{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4: unconstrained optimization- gradient-based methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Optimization problem to be studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still studying functions of multiple variables in unconstrained optimization problems\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.}\\quad &x\\in \\mathbb R^n\n",
    "\\end{align}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us define the same function as on the previous lecture for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & (x_1-10)^2+(x_2+5)^2+x_1^2\\\\\n",
    "\\text{s.t.}\\quad &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: Model algorithm for unconstrained minimization (an iterative approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let $x_k$ be the current approximation for a local minimum $x^*$:\n",
    " \n",
    "* (1) **Check the convergency** If conditions are satisfied, stop. The solution is $x_k$.\n",
    "\n",
    "* (2) **Identify a search direction**  Find a nonzero vector $d_k \\in \\mathbb R^n$ as a search direction.\n",
    "\n",
    "* (3) **Identify a step length** Find an $\\alpha_k > 0$ as a step length, for which $f(x_k + \\alpha_k d_k) < f(x_k)$ (i.e., leads us to a better approximation)\n",
    "\n",
    "* (4) **Update the current approximation for the local minimum** Set $x_{k+1} = x_k + \\alpha_k d_k, k =  k + 1$ and go to (1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient-based optimization\n",
    "\n",
    "If gradients are available, they should always be used --> improve convergency\n",
    "\n",
    "Gradient-based methods usually converge to a local minimum which is nearest to the starting point (**Why?**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If analytical formulas of the gradients are not available, one way is to numerically approximate them by using finite differences ($‚Ñé>0$)\n",
    "* forward difference: $\\frac{f(x+h)-f(x)}{h}=\\nabla f(x)+O(h)$\n",
    "* central difference: $\\frac{f(x+h)-f(x-h)}{2h}=\\nabla f(x)+O(h^2)$\n",
    "* central difference is more accurate but requires twice as many function evaluations (in $ùë•+‚Ñé$ and $ùë•‚àí‚Ñé$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation in Python\n",
    "\n",
    "An alternative way is to use automatic differentiation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Automatic_differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Import automatic differentiation package for Python\n",
    "\n",
    "Needs to be installed typing\n",
    "```\n",
    "(!) pip install ad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can ask for gradient and hessian using the <pre>ad.gh</pre> function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Let us use automatic differentiation for the function <it>f</it>  that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "grad_f, hess_f = ad.gh(f_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At the point (1,2) gradient is \", grad_f([1,2]), \" and hessian is \",hess_f([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let us visualize the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "def visualize_gradient(f,point,x_lim,y_lim):\n",
    "    grad_point = np.array(ad.gh(f)[0](point)) # computes the gradient\n",
    "    grad_point = 10.0*grad_point/np.linalg.norm(grad_point) # scale the gradient vector so that it is easier to see\n",
    "    X,Y,Z = point[0],point[1],f(point) # point that we are interested in and the function value in that point \n",
    "    U,V,W = grad_point[0],grad_point[1],0 # gradient vector, last component is zero since the gradient has 2 components\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d') \n",
    "    x = np.arange(x_lim[0],x_lim[1],0.1) # divide interval for x\n",
    "    y = np.arange(y_lim[0],y_lim[1],0.1) # divide interval for y\n",
    "    X2,Y2 = meshgrid(x, y) # make a grid for x and y\n",
    "    Z2 = [f([x,y]) for (x,y) in zip (X2,Y2)] # evaluation of the function on the grid\n",
    "    Z2 = np.asarray(Z2)\n",
    "    surf = ax.plot_surface(X2, Y2, Z2,alpha=0.5) # surface plot for the function values\n",
    "    ax.quiver(X,Y,Z,U,V,W,color='red',linewidth=2.5) # plots a 2d arrow in a 3d figure\n",
    "    return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_gradient(f_simple,[1,-2],[0,10],[-10,0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the lambda function we can easily visualize gradients of various functions without a user-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "visualize_gradient(lambda x:5*x[0]+3*x[1],[1,1],[0,2],[0,2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient-based Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steepest descent and Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reminder: Model algorithm for unconstrained minimization (an iterative approach)\n",
    "\n",
    "Let $x_k$ be the current approximation for a local minimum $x^*$:\n",
    " \n",
    "* (1) **Check the convergency** If conditions are satisfied, stop. The solution is $x_k$.\n",
    "\n",
    "* (2) **Identify a search direction**  Find a nonzero vector $d_k \\in \\mathbb R^n$ as a search direction.\n",
    "\n",
    "* (3) **Identify a step length** Find a $\\alpha_k > 0$ as a step length, for which $f(x_k + \\alpha_k d_k) < f(x_k)$ (i.e., leads us to a better approximation)\n",
    "\n",
    "* (4) **Update the current approximation for the local minimum** Set $x_{k+1} = x_k + \\alpha_k d_k, k =  k + 1$ and go to (1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Base algorithm for the steepest descent and Newton's algorithms\n",
    "**Input:** function $f$ to be optimized, starting point $x_0$, step length rule $alpha$, stopping rule $stop$  \n",
    "**Output:** A solution $x^*$ that is close to a locally optimal solution\n",
    "```\n",
    "set f_old as a big number and f_new as f(x0)\n",
    "while a stopping criterion has not been met:\n",
    "    f_old = f_new\n",
    "    determine search direction d_k according to the method\n",
    "    determine the step length alpha\n",
    "    set x = x + alpha *d_k\n",
    "    f_new = f(x)\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to determine search direction distinguishes steepest descent algorithm and the Newton algorithm. Different stopping rules and step sizes can be mixed and matched with both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When moving along the gradient direction from any point to n-dimensional space, the function values change at the **fastest rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steepest Descent algorithm for unconstrained optimization\n",
    "\n",
    "Also known as *gradient descent* or *gradient method*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Desent direction:\n",
    "\n",
    "* If $d_k$ is a descent direction for $f$ at $x_k$: $f(x_{k+1}) < f(x_k) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, if $x_{k+1} = x_k + \\alpha_k d_k$:\n",
    "\n",
    "* $\\rightarrow  f(x_k + \\alpha_k d_k) < f(x_k)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using Taylor series expansion of $f$:\n",
    "\n",
    "* $\\rightarrow  f(x_k) + \\alpha_k ( \\nabla f(x_k) . d_k) < f(x_k) $ \n",
    "\n",
    "\n",
    "* $\\rightarrow \\alpha_k ( \\nabla f(x_k) . d_k) < 0 $, ($\\alpha_k > 0$)\n",
    "\n",
    "\n",
    "* $\\rightarrow \\nabla f(x_k) . d_k < 0 $ (**condition for the descent direction**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steepest Descent algorithm for unconstrained optimization\n",
    "\n",
    "Also known as *gradient descent* or *gradient method*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the steepest descent algorithm, the search direction $d_k$ is determined by the negative of the gradient $-\\nabla f(x_k)$ which (locally) gives the direction of the steepest descent of $f$ at point $x_k$.\n",
    "\n",
    "* $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Indeed, the objective function decreases fastest in $x_k$ (you can check it out yourself using the condition for the descent direction if you are interested)\n",
    " \n",
    " * Very simple example of a univariant function\n",
    " ![](images/2d-grad-desc.jpg)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " An example of a function with two variables:\n",
    " \n",
    " ![](images/2d-grad-desc1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another example of a function with two variables:\n",
    " \n",
    " ![](images/2d-grad-desc2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple stopping rule, where we stop when the change is not bigger than precision and we have a fixed step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "    #while np.linalg.norm(ad.gh(f)[0](x))>precision: # an alternative stopping rule\n",
    "        f_old = f_new # store value at the current point\n",
    "        d = -np.array(ad.gh(f)[0](x)) # search direction\n",
    "        x = x+d*step # take a step\n",
    "        f_new = f(x) # compute function value at the new point\n",
    "        steps.append(list(x)) # save step\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solve the problem using the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "step_size = 0.1\n",
    "precision = 0.01\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot the steps of solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    #print(myvec)\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other stopping rules\n",
    "* change in function values: $|f_{new}-f_{old}| \\leq $precision \n",
    "\n",
    "This one we already used. Others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* norm of the gradient: $||\\nabla f(x_{new})|| \\leq $precision\n",
    "* change in variable values: $||x_{new}-x_{old}|| \\leq $precision\n",
    "* maximum number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Another example\n",
    "def f_simple2(x):\n",
    "    return (x[0] - 2.0)**4 + (x[0] - 2.0*x[1])**2\n",
    "# Optimal solution is (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "step_size = .01\n",
    "precision = 0.0001\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple2,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))\n",
    "#print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compare steepest descent with optimized step size:\n",
    "![](images/steepest_descent.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on setting the search direction as $d_k=-[H(x_k)]^{-1}\\nabla f(x_k).\\hspace{2.0cm}$   --> **uses second derivatives!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In one-dimensional case by the Taylor series:\n",
    "$$f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x+\\frac12f''(x)\\Delta x^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to find $x$ such that $f(x)$ is at minimum and, thus, we seek to solve the equation that sets the derivative of this expression with respect to $\\Delta x$ equal to zero (Necessary condition):\n",
    "\n",
    "$$ 0 = \\frac{d}{d\\Delta x} \\left(f(x_k)+f'(x_k)\\Delta x+\\frac 1 2 f''(x_k) \\Delta x^2\\right) = f'(x_k)+f'' (x_k) \\Delta x.$$\n",
    "\n",
    "The solution of the above equation is $\\Delta x=-f'(x_k)/f''(x_k)$. Thus, the best approximation of $x_{k+1}$ as the minimum is $x_k-f''(x_k)^{-1}f'(x_k)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def newton(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new # store value of f at the current point\n",
    "        H_inv = np.linalg.inv(np.matrix(ad.gh(f)[1](x))) # compute the inverse of the Hessian\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose() # compute search direction\n",
    "        #Change the type from np.matrix to np.array so that we can use it in our function\n",
    "        x = np.array(x+d*step)[0] # take the step\n",
    "        f_new = f(x) # compute function value at the new point\n",
    "        steps.append(list(x)) # store step\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "step_size = 0.5\n",
    "precision = 0.001\n",
    "(x_value,f_value,steps) = newton(f_simple,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "step_size = 0.3\n",
    "precision = 0.001\n",
    "(x_value,f_value,steps) = newton(f_simple2,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compare Newton's method with step size = 1.0:\n",
    "![](images/newton.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "Steepest descent\n",
    "* Descent direction (does the search direction guarantee decrease in $f$): yes\n",
    "* Global convergence (converges from any starting point): yes\n",
    "* Local convergence: zig-zag near optimum\n",
    "* Computational bottle neck: step length\n",
    "* Memory consumption: $ùëÇ(ùëõ)$\n",
    "\n",
    "Newton‚Äôs method\n",
    "* Descent direction: only if $ùêª(ùë•)^{‚àí1}$ positive definite\n",
    "* Global convergence: no\n",
    "* Local convergence: yes, good if ùëì quadratic\n",
    "* Computational bottle neck: $ùêª(ùë• )^{‚àí1}$ --> can be relaxed by using approximations of $H(x)^{-1}$ --> quasi-Newton\n",
    "* Memory consumption: $ùëÇ(ùëõ^2)$ --> can be reduced e.g. by using conjugate gradient methods"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
